# ğŸ§  Ollama lokal mit Docker nutzen

Dieses Setup beschreibt, wie du [Ollama](https://ollama.com/) lokal auf deinem Raspberry Pi (oder einem anderen Linux-System) im Docker-Container betreibst und per API ansteuerst.

## ğŸ“ Voraussetzungen

* Docker & Docker Compose
* ca. 4â€“8 GB RAM empfohlen (je nach Modell)
* optional: Reverse Proxy (z.â€¯B. Caddy)

## ğŸ§° Schritt-fÃ¼r-Schritt

### 1. `docker-compose.ollama.yml` erstellen

```yaml
disversion: '3.8'
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
volumes:
  ollama:
```

> Port 11434 ist der Standardport der Ollama-API.

### 2. Container starten

```bash
docker compose -f docker-compose.ollama.yml up -d
```

### 3. Modell herunterladen

Beispiel fÃ¼r TinyLlama:

```bash
curl http://localhost:11434/api/pull -d '{"name": "tinyllama"}'
```

Alternativ via UI (nur verfÃ¼gbar, wenn aktiviert).

## ğŸ§ª Testen

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "tinyllama",
  "prompt": "Was ist der Sinn des Lebens?"
}'
```

## ğŸ“ Beispielverzeichnis

```bash
n8n-server/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.ollama.yml         # â† optionaler Container fÃ¼r Ollama
â””â”€â”€ docs/
    â””â”€â”€ setup-ollama-local.md         # â† diese Datei
```

## ğŸ“„ WeiterfÃ¼hrend

* [Ollama Docs](https://ollama.com/library)
* [API Referenz](https://ollama.com/docs/api)
