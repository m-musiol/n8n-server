# ðŸ“š Installierte Modelle (Ollama)

Dieses Dokument listet die speziell fÃ¼r diesen Anwendungszweck installierten KI-Modelle im Docker-Ollama-Setup auf. Die Modelle wurden manuell per API gezogen und stehen Ã¼ber den Ollama-Server unter `http://localhost:11434` zur VerfÃ¼gung.

## ðŸ“¦ Aktuell installierte Modelle

### 1. `gemma:2b`

* **ModellgrÃ¶ÃŸe:** ca. 1,7â€¯GB
* **Typ:** Decoder-only, Chat-optimiert
* **Sprache:** Multilingual (inkl. Deutsch)
* **Vorteile:**

  * Gute Balance aus Geschwindigkeit und SprachqualitÃ¤t
  * LauffÃ¤hig auf Raspberry Piâ€¯5 (8â€¯GB RAM empfohlen)
  * Lizenz von Google erlaubt Forschung und private Nutzung
* **Einsatz:** Allgemeine Texteingabe, einfache Dialogsysteme

**Beispielbefehl zur Installation:**

```bash
docker exec -it ollama ollama run gemma:2b
```

### 2. `tinyllama`

* **ModellgrÃ¶ÃŸe:** ca. 640â€¯MB
* **Typ:** Kleinstes lauffÃ¤higes LLM mit brauchbarer Sprachleistung
* **Sprache:** Englisch (eingeschrÃ¤nkt auch andere Sprachen)
* **Vorteile:**

  * Extrem leichtgewichtig
  * Ideal fÃ¼r ressourcenarme Systeme wie den Piâ€¯5
  * Schnellere Antwortzeiten
* **EinschrÃ¤nkungen:**

  * Begrenzte KontextlÃ¤nge
  * SprachqualitÃ¤t niedriger als bei grÃ¶ÃŸeren Modellen
* **Einsatz:** Tests, einfache Promptbeispiele, Skriptgenerierung

**Beispielbefehl zur Installation:**

```bash
docker exec -it ollama ollama run tinyllama
```

## ðŸ“ Speicherort

Die Modelle liegen standardmÃ¤ÃŸig im Docker-Volume `ollama`, das beim Starten des Containers automatisch angelegt wird:

```bash
volumes:
  - ollama:/root/.ollama
```

## ðŸ” Modellpflege

### Aktualisieren eines Modells

```bash
curl http://localhost:11434/api/pull -d '{"name": "tinyllama"}'
```

### Modell entfernen (wenn mÃ¶glich)

> Derzeit ist das Entfernen nicht direkt dokumentiert. Container-Reset entfernt auch Modelle.

## ðŸ“„ Referenzen

* [Ollama Model Registry](https://ollama.com/library)
* [API Referenz](https://ollama.com/docs/api)
